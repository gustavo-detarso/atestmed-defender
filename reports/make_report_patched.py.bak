#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Relatório ATESTMED — Individual ou Top 10
Executa a geração de gráficos (Python), análises (R), junta comentários e
monta relatórios em Org/PDF. Inclui panorama weekday→weekend e apêndices.
"""

# ────────────────────────────────────────────────────────────────────────────────
# Imports (todos consolidados no início)
# ────────────────────────────────────────────────────────────────────────────────
import os
import sys
import re
import csv
import shutil
import sqlite3
import subprocess
import tempfile
import time
import calendar
from glob import glob
from datetime import datetime, date
from collections import defaultdict

import pandas as pd
from PyPDF2 import PdfMerger

# ────────────────────────────────────────────────────────────────────────────────
# Paths e diretórios
# ────────────────────────────────────────────────────────────────────────────────
BASE_DIR    = os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))
DB_PATH     = os.path.join(BASE_DIR, 'db', 'atestmed.db')
GRAPHS_DIR  = os.path.join(BASE_DIR, 'graphs_and_tables')
SCRIPTS_DIR = GRAPHS_DIR  # alias
EXPORT_DIR  = os.path.join(GRAPHS_DIR, 'exports')
OUTPUTS_DIR = os.path.join(BASE_DIR, 'reports', 'outputs')
MISC_DIR    = os.path.join(BASE_DIR, 'misc')
RCHECK_DIR  = os.path.join(BASE_DIR, 'r_checks')  # onde ficam os .R

os.makedirs(EXPORT_DIR, exist_ok=True)
os.makedirs(OUTPUTS_DIR, exist_ok=True)

# ────────────────────────────────────────────────────────────────────────────────
# Carregamento de .env na raiz (se existir)
# ────────────────────────────────────────────────────────────────────────────────
def _load_env_from_root():
    """Carrega variáveis do arquivo .env na raiz do projeto (sem sobrescrever o ambiente)."""
    env_path = os.path.join(BASE_DIR, ".env")
    try:
        from dotenv import load_dotenv
        load_dotenv(env_path, override=False)
        return
    except Exception:
        pass
    if os.path.exists(env_path):
        with open(env_path, "r", encoding="utf-8") as f:
            for line in f:
                line = line.strip()
                if not line or line.startswith("#") or "=" not in line:
                    continue
                k, v = line.split("=", 1)
                v = v.strip().strip('"').strip("'")
                if v:
                    os.environ.setdefault(k.strip(), v)

_load_env_from_root()
if not os.getenv("OPENAI_API_KEY"):
    print("⚠️  OPENAI_API_KEY não encontrado no ambiente (.env na raiz não carregou ou não tem a chave).")

# ────────────────────────────────────────────────────────────────────────────────
# Ordem padrão dos scripts Python e configurações
# ────────────────────────────────────────────────────────────────────────────────
SCRIPT_ORDER = [
    "compare_nc_rate.py",
    "compare_motivos_perito_vs_brasil.py",
    "compare_productivity.py",
    "compare_fifteen_seconds.py",
    "compare_overlap.py",
    "compare_indicadores_composto.py",
]

DEFAULT_MODES = {
    "compare_productivity.py": ["perito-share", "task-share", "time-share"],
    "compare_overlap.py":      ["perito-share", "task-share", "time-share"],
}

EXTRA_ARGS = {
    "compare_motivos_perito_vs_brasil.py": ["--label-maxlen", "14", "--label-fontsize", "7"],
}

PRODUCTIVITY_THRESHOLD = "50"  # análises/h
FIFTEEN_THRESHOLD      = "15"  # segundos
FIFTEEN_CUT_N          = "10"  # mínimo de análises ≤ threshold

# ────────────────────────────────────────────────────────────────────────────────
# Scripts globais (por período; independem de perito/top10)
# ────────────────────────────────────────────────────────────────────────────────
GLOBAL_SCRIPTS = [
    "g_weekday_to_weekend_table.py",
]

def build_commands_for_global(script_file: str, start: str, end: str) -> list:
    """Monta comando para scripts globais que só precisam de --start/--end."""
    return [[
        sys.executable, script_file,
        "--db", DB_PATH,
        "--start", start,
        "--end", end,
        "--out-dir", EXPORT_DIR,
        "--export-org",
        "--export-png",
        "--export-protocols",
    ]]

# ────────────────────────────────────────────────────────────────────────────────
# R checks — individuais e top10
# ────────────────────────────────────────────────────────────────────────────────
RCHECK_SCRIPTS = [
    ("01_nc_rate_check.R",          {"need_perito": True}),
    ("02_le15s_check.R",            {"need_perito": True, "defaults": {"--threshold": FIFTEEN_THRESHOLD}}),
    ("03_productivity_check.R",     {"need_perito": True, "defaults": {"--threshold": PRODUCTIVITY_THRESHOLD}}),
    ("04_overlap_check.R",          {"need_perito": True}),
    ("05_motivos_chisq.R",          {"need_perito": True}),
    ("06_composite_robustness.R",   {"need_perito": True}),
    ("07_kpi_icra_iatd_score.R",    {"need_perito": True}),
    ("08_weighted_props.R",         {"need_perito": True, "defaults": {"--measure": "nc"}}),
    ("08_weighted_props.R",         {"need_perito": True, "defaults": {"--measure": "le", "--threshold": FIFTEEN_THRESHOLD}}),
]

RCHECK_GROUP_SCRIPTS = [
    ("g01_top10_nc_rate_check.R",        {"defaults": {}}),
    ("g02_top10_le15s_check.R",          {"defaults": {"--threshold": FIFTEEN_THRESHOLD}}),
    ("g03_top10_productivity_check.R",   {"defaults": {"--threshold": PRODUCTIVITY_THRESHOLD}}),
    ("g04_top10_overlap_check.R",        {"defaults": {}}),
    ("g05_top10_motivos_chisq.R",        {"defaults": {}}),
    ("g06_top10_composite_robustness.R", {"defaults": {}}),
    ("g07_top10_kpi_icra_iatd_score.R",  {"defaults": {}}),
    ("08_weighted_props.R",              {"pass_top10": True, "defaults": {"--measure": "nc"}}),
    ("08_weighted_props.R",              {"pass_top10": True, "defaults": {"--measure": "le", "--threshold": FIFTEEN_THRESHOLD}}),
]

# Comentário ChatGPT para apêndice R (opcional)
try:
    from utils.comentarios import comentar_r_apendice
except Exception:
    comentar_r_apendice = None

# ────────────────────────────────────────────────────────────────────────────────
# Conhecimento explícito das flags dos scripts Python
# ────────────────────────────────────────────────────────────────────────────────
ASSUME_FLAGS = {
    "compare_nc_rate.py": {
        "--perito", "--nome", "--top10", "--min-analises",
        "--export-md", "--export-png", "--export-org",
        "--export-comment", "--export-comment-org", "--call-api",
    },
    "compare_fifteen_seconds.py": {
        "--perito", "--nome", "--top10", "--min-analises",
        "--threshold", "--cut-n",
        "--export-png", "--export-org",
        "--export-comment", "--export-comment-org", "--call-api",
    },
    "compare_overlap.py": {
        "--perito", "--nome", "--top10", "--min-analises",
        "--mode", "--chart",
        "--export-md", "--export-png", "--export-org",
        "--export-comment", "--export-comment-org", "--call-api",
    },
    "compare_productivity.py": {
        "--perito", "--nome", "--top10", "--min-analises",
        "--threshold", "--mode", "--chart",
        "--export-md", "--export-png", "--export-org",
        "--export-comment", "--export-comment-org", "--call-api",
    },
    "compare_indicadores_composto.py": {
        "--perito", "--top10", "--min-analises",
        "--alvo-prod", "--cut-prod-pct", "--cut-nc-pct", "--cut-le15s-pct", "--cut-overlap-pct",
        "--export-png", "--export-org", "--chart",
        "--export-comment", "--export-comment-org", "--call-api",
    },
    "compare_motivos_perito_vs_brasil.py": {
        "--perito", "--top10", "--min-analises",
        "--topn", "--min-pct-perito", "--min-pct-brasil", "--min-n-perito", "--min-n-brasil",
        "--label-maxlen", "--label-fontsize",
        "--chart", "--export-md", "--export-org", "--export-png",
        "--export-comment", "--export-comment-org", "--call-api",
    },
}

# ────────────────────────────────────────────────────────────────────────────────
# CLI
# ────────────────────────────────────────────────────────────────────────────────
def parse_args():
    """Parser de argumentos de linha de comando."""
    import argparse
    p = argparse.ArgumentParser(
        description=("Relatório ATESTMED — Individual ou Top 10 "
                     "(executa gráficos e R checks; monta Org/PDF; "
                     "inclui panorama weekday→weekend).")
    )
    p.add_argument('--start', required=True, help='Data inicial YYYY-MM-DD')
    p.add_argument('--end',   required=True, help='Data final   YYYY-MM-DD')

    who = p.add_mutually_exclusive_group(required=True)
    who.add_argument('--perito', help='Nome do perito (relatório individual)')
    who.add_argument('--top10', action='store_true', help='Gera relatório para os 10 piores peritos do período')

    p.add_argument('--min-analises', type=int, default=50, help='Mínimo de análises para elegibilidade do Top 10')

    p.add_argument('--include-high-nc', dest='include_high_nc', action='store_true', default=True,
                   help='(default) Incluir coorte extra de %NC alta no relatório Top 10')
    p.add_argument('--no-high-nc', dest='include_high_nc', action='store_false',
                   help='Não incluir a coorte extra de %NC alta')
    p.add_argument('--high-nc-threshold', type=float, default=90.0, help='Limiar de %NC para a coorte extra (padrão: 90)')
    p.add_argument('--high-nc-min-tasks', type=int, default=50, help='Mínimo de tarefas para a coorte extra (padrão: 50)')

    p.add_argument('--export-org', action='store_true', help='Exporta relatório consolidado em Org-mode')
    p.add_argument('--export-pdf', action='store_true', help='Exporta relatório consolidado em PDF (via Pandoc)')
    p.add_argument('--add-comments', action='store_true', help='Inclui comentários GPT (quando suportado pelos scripts)')

    p.add_argument('--plan-only', action='store_true', help='Somente imprime os comandos planejados (dry-run)')

    p.add_argument('--r-appendix', action='store_true', default=True, help='(default) Executa os R checks e inclui no apêndice')
    p.add_argument('--no-r-appendix', dest='r_appendix', action='store_false', help='Não executar os R checks')
    p.add_argument('--r-bin', default='Rscript', help='Binário do Rscript (padrão: Rscript)')
    return p.parse_args()

# ────────────────────────────────────────────────────────────────────────────────
# Helpers gerais
# ────────────────────────────────────────────────────────────────────────────────
def _safe(name: str) -> str:
    """Sanitiza nome para uso em arquivos/caminhos."""
    return "".join(c if c.isalnum() or c in ("-","_") else "_" for c in str(name)).strip("_") or "output"

def _env_with_project_path():
    """Garante BASE_DIR no PYTHONPATH ao chamar scripts Python filhos."""
    env = os.environ.copy()
    py = env.get("PYTHONPATH", "")
    parts = [BASE_DIR] + ([py] if py else [])
    env["PYTHONPATH"] = os.pathsep.join(parts)
    return env

def script_path(name: str) -> str:
    """Resolve caminho absoluto de um script Python do pacote."""
    path = os.path.join(SCRIPTS_DIR, name)
    if not os.path.isfile(path):
        raise FileNotFoundError(f"Script não encontrado: {path}")
    return path

def rscript_path(name: str) -> str:
    """Resolve caminho absoluto de um script R do pacote."""
    path = os.path.join(RCHECK_DIR, name)
    if not os.path.isfile(path):
        raise FileNotFoundError(f"R script não encontrado: {path}")
    return path

def introspect_script(script_file: str) -> dict:
    """Executa --help do script para descobrir flags e modos disponíveis."""
    info = {"flags": set(), "modes": []}
    try:
        out = subprocess.run(
            [sys.executable, script_file, "--help"],
            capture_output=True, text=True, env=_env_with_project_path(), cwd=SCRIPTS_DIR
        )
        text = (out.stdout or "") + "\n" + (out.stderr or "")
        for m in re.finditer(r"(--[a-zA-Z0-9][a-zA-Z0-9\-]*)", text):
            info["flags"].add(m.group(1))
        mm = re.search(r"--mode[^\n]*\{([^}]+)\}", text)
        if mm:
            info["modes"] = [x.strip() for x in mm.group(1).split(",") if x.strip()]
    except Exception:
        pass
    return info

def detect_modes(script_file: str, help_info: dict) -> list:
    """Retorna a lista de modos suportados pelo script (via --help ou DEFAULT_MODES)."""
    name = os.path.basename(script_file)
    modes = help_info.get("modes") or []
    return modes if modes else DEFAULT_MODES.get(name, [])
    
def _inject_comment_for_stem(stem, comments_dir, output_dir, imgs_dir=None):
    """
    Ordem de busca:
      (A) comments_dir → *.org (usa conteúdo inteiro) → *.md (converte p/ org)
      (B) EXPORT_DIR   → *.org (extrai 1º parágrafo via _extract_comment_from_org)
      (C) output_dir/orgs → *.org (extrai 1º parágrafo)
      (D) imgs_dir     → *.org (extrai 1º parágrafo)

    Retorna ['#+BEGIN_QUOTE', ..., '#+END_QUOTE'] ou [].
    """
    def _quote_block(txt):
        return ["", "#+BEGIN_QUOTE", _protect_tables_in_quote(txt.strip()), "#+END_QUOTE"]

    # (A) comments_dir — .org preferencial; depois .md
    if comments_dir and os.path.isdir(comments_dir):
        for orgc in (os.path.join(comments_dir, f"{stem}_comment.org"),
                     os.path.join(comments_dir, f"{stem}.org")):
            if os.path.exists(orgc):
                with open(orgc, encoding="utf-8") as f:
                    comment_org = f.read().strip()
                print(f"[comments] .org (comments_dir) → {orgc}")
                return _quote_block(comment_org)

        for md in (os.path.join(comments_dir, f"{stem}_comment.md"),
                   os.path.join(comments_dir, f"{stem}.md")):
            if os.path.exists(md):
                with open(md, encoding="utf-8") as f:
                    comment_md = f.read().strip()
                print(f"[comments] .md (comments_dir) → {md}")
                org_txt = markdown_para_org(comment_md)
                org_txt = "\n".join(
                    ln for ln in org_txt.splitlines()
                    if not ln.strip().lower().startswith('#+title')
                ).strip()
                return _quote_block(org_txt)

    # (B) EXPORT_DIR — .org auxiliar (ex.: produtividade_perito-share_50h_<NOME>.org)
    export_dir = globals().get("EXPORT_DIR", None)
    if export_dir and os.path.isdir(export_dir):
        for orgc in (os.path.join(export_dir, f"{stem}.org"),
                     os.path.join(export_dir, f"{stem}_comment.org")):
            if os.path.exists(orgc):
                with open(orgc, encoding="utf-8") as f:
                    aux_org = f.read()
                extra = _extract_comment_from_org(aux_org)
                if extra:
                    print(f"[comments] extraído do EXPORT_DIR → {orgc}")
                    return _quote_block(extra)

    # (C) output_dir/orgs — .org auxiliar salvo junto ao relatório
    aux_org_path = os.path.join(output_dir, "orgs", f"{stem}.org")
    if os.path.exists(aux_org_path):
        with open(aux_org_path, encoding="utf-8") as f:
            aux_org = f.read()
        extra = _extract_comment_from_org(aux_org)
        if extra:
            print(f"[comments] extraído de output_dir/orgs → {aux_org_path}")
            return _quote_block(extra)
        else:
            print(f"[comments] org auxiliar sem parágrafo extraível → {aux_org_path}")

    # (D) imgs_dir — .org ao lado do PNG (caso raro)
    if imgs_dir and os.path.isdir(imgs_dir):
        local_org = os.path.join(imgs_dir, f"{stem}.org")
        if os.path.exists(local_org):
            with open(local_org, encoding="utf-8") as f:
                aux_org = f.read()
            extra = _extract_comment_from_org(aux_org)
            if extra:
                print(f"[comments] extraído de imgs_dir → {local_org}")
                return _quote_block(extra)

    print(f"[comments] comentário não encontrado para stem='{stem}'")
    return []


# ────────────────────────────────────────────────────────────────────────────────
# Weekday→Weekend: consultas e inserções em Org
# ────────────────────────────────────────────────────────────────────────────────
def _detect_end_datetime_column(conn) -> str | None:
    """Detecta o nome da coluna de término na tabela 'analises'."""
    cols = [r[1] for r in conn.execute("PRAGMA table_info(analises)").fetchall()]
    for cand in ("dataHoraFimPericia", "dataHoraFimAnalise", "dataHoraFim"):
        if cand in cols:
            return cand
    return None

def _weekday2weekend_protocols_by_perito(start: str, end: str):
    """
    Retorna (lista_ordenada, total) de protocolos: início em dia útil e conclusão no fim de semana.
    lista_ordenada = [(perito, [protocolos...]), ...]
    """
    conn = sqlite3.connect(DB_PATH)
    end_col = _detect_end_datetime_column(conn)
    if not end_col:
        conn.close()
        print("[AVISO] Coluna de término não encontrada na tabela 'analises'.")
        return [], 0

    sql = f"""
        SELECT p.nomePerito AS perito, a.protocolo AS protocolo
          FROM analises a
          JOIN peritos p ON a.siapePerito = p.siapePerito
         WHERE date(a.dataHoraIniPericia) BETWEEN ? AND ?
           AND CAST(strftime('%w', date(a.dataHoraIniPericia)) AS INTEGER) BETWEEN 1 AND 5
           AND a.{end_col} IS NOT NULL
           AND CAST(strftime('%w', date(a.{end_col})) AS INTEGER) IN (0,6)
         ORDER BY p.nomePerito, a.protocolo
    """
    rows = conn.execute(sql, (start, end)).fetchall()
    conn.close()

    por_perito = {}
    for perito, protocolo in rows:
        por_perito.setdefault(perito, []).append(str(protocolo))

    ordenado = sorted(por_perito.items(), key=lambda kv: (-len(kv[1]), kv[0]))
    total = len(rows)
    return ordenado, total

def _append_weekday2weekend_panorama_block(
    lines: list,
    imgs_dir: str,
    comments_dir: str,
    start: str = None,
    end: str = None,
    heading_level: str = "**"
) -> bool:
    """
    Acrescenta panorama global (gráfico por CR, tabela org e protocolos por perito), se existirem.
    """
    orgs_dir   = os.path.join(os.path.dirname(imgs_dir), "orgs")
    png_path   = os.path.join(imgs_dir, "rcheck_weekday_to_weekend_by_cr.png")
    table_org  = os.path.join(orgs_dir, "rcheck_weekday_to_weekend_table.org")
    protos_org = os.path.join(orgs_dir, "rcheck_weekday_to_weekend_protocols.org")
    comment_org= os.path.join(comments_dir, "rcheck_weekday_to_weekend_table_comment.org")

    found_any = any(os.path.exists(p) for p in (png_path, table_org, protos_org, comment_org))
    if not found_any:
        return False

    lines.append(f"{heading_level} Panorama global — Início em dia útil → conclusão no fim de semana (por CR)")

    # Comentário (se existir)
    if os.path.exists(comment_org):
        try:
            with open(comment_org, encoding="utf-8") as f:
                ctext = f.read().strip()
            ctext = _protect_tables_in_quote(ctext)
            lines.extend(["", "#+BEGIN_QUOTE", ctext, "#+END_QUOTE"])
        except Exception as e:
            print(f"[AVISO] Falha ao anexar comentário do panorama: {e}")

    # Tabela (+ possível figura embutida)
    table_content = ""
    has_png_inside_table = False
    if os.path.exists(table_org):
        try:
            with open(table_org, encoding="utf-8") as f:
                table_content = f.read().strip()
            table_content = "\n".join(
                ln for ln in table_content.splitlines()
                if not ln.strip().lower().startswith("#+title")
            ).strip()
            has_png_inside_table = "rcheck_weekday_to_weekend_by_cr.png" in table_content
            table_content = _ensure_blank_lines_around_tables(table_content)
        except Exception as e:
            print(f"[AVISO] Falha ao ler tabela do panorama: {e}")
            table_content = ""

    if os.path.exists(png_path) and not has_png_inside_table:
        base = os.path.basename(png_path)
        lines.extend([
            "",
            "#+ATTR_LATEX: :placement [H] :width \\linewidth",
            "#+CAPTION: Tarefas iniciadas em dia útil e concluídas no fim de semana — por CR (ordenado)",
            f"[[file:imgs/{base}]]",
        ])

    if table_content:
        lines.extend(["", table_content])

    # Protocolos: .org pronto ou fallback dinâmico
    appended_protocols = False
    if os.path.exists(protos_org):
        try:
            with open(protos_org, encoding="utf-8") as f:
                protos_content = f.read().strip()
            protos_content = "\n".join(
                ln for ln in protos_content.splitlines()
                if not ln.strip().lower().startswith("#+title")
            ).strip()
            protos_content = re.sub(
                r'^\s*\*+\s+Protocolos envolvidos\s*\(por perito\)\s*\n',
                '',
                protos_content,
                count=1,
                flags=re.IGNORECASE
            ).lstrip()
            lines.append("")
            lines.append(f"{heading_level} Protocolos envolvidos (por perito)")
            try:
                protos_content = shift_org_headings(protos_content, delta=1)
            except NameError:
                pass
            lines.append(protos_content)
            appended_protocols = True
        except Exception as e:
            print(f"[AVISO] Falha ao anexar protocolos (.org): {e}")

    if not appended_protocols and start and end:
        try:
            lista, total = _weekday2weekend_protocols_by_perito(start, end)
            if lista:
                lines.append("")
                lines.append(f"{heading_level} Protocolos envolvidos (por perito)")
                for perito, protos in lista:
                    lines.append(f"- *{perito}* ({len(protos)}): {', '.join(protos)}")
                lines.append(f"\n- **Total de protocolos:** {total}")
        except Exception as e:
            print(f"[AVISO] Falha no fallback dinâmico de protocolos: {e}")

    lines.append("\n#+LATEX: \\newpage\n")
    return True

def _append_weekday2weekend_perito_block_if_any(
    lines: list,
    perito: str,
    imgs_dir: str,
    comments_dir: str,
    start: str,
    end: str,
    heading_level: str = "**",
) -> bool:
    """
    Adiciona, AO FINAL do relatório individual, um bloco com os casos weekday→weekend
    somente se o perito tiver pelo menos 1 protocolo.
    """
    orgs_dir   = os.path.join(os.path.dirname(imgs_dir), "orgs")
    table_org  = os.path.join(orgs_dir, "rcheck_weekday_to_weekend_table.org")
    protos_org = os.path.join(orgs_dir, "rcheck_weekday_to_weekend_protocols.org")

    protos = _read_protocols_for_perito_from_org(protos_org, perito) if os.path.exists(protos_org) else []
    if not protos:
        try:
            lst, _total = _weekday2weekend_protocols_by_perito(start, end)
            perito_lower = perito.strip().lower()
            for p, pnums in lst:
                if p.strip().lower() == perito_lower:
                    protos = list(pnums)
                    break
        except Exception as e:
            print(f"[AVISO] Fallback dinâmico de protocolos falhou: {e}")

    if not protos:
        return False

    lines.append(f"{heading_level} Início em dia útil → conclusão no fim de semana (este perito)")
    lines.append(f"Janela: {start} a {end}. Somente tarefas deste perito iniciadas em dia útil e concluídas no fim de semana.\n")

    mini_table = _extract_single_row_from_org_table(table_org, perito) if os.path.exists(table_org) else ""
    if mini_table:
        lines.append("#+CAPTION: Resumo (apenas este perito)")
        lines.append(mini_table)
        lines.append("")

    lines.append("#+CAPTION: Protocolos deste perito")
    protos_sorted = sorted(protos)
    chunk, max_len = [], 25
    while protos_sorted:
        chunk, protos_sorted = protos_sorted[:max_len], protos_sorted[max_len:]
        lines.append(f"- {', '.join(chunk)}")
    lines.append("\n#+LATEX: \\newpage\n")
    return True

def _read_protocols_for_perito_from_org(path: str, perito: str) -> list:
    """Extrai lista de protocolos do perito a partir do .org de protocolos."""
    if not os.path.exists(path):
        return []
    perito_low = perito.strip().lower()
    out = []
    try:
        with open(path, encoding="utf-8") as f:
            for raw in f:
                line = raw.strip()
                if ":" not in line:
                    continue
                l = line.lstrip("-• ").strip()
                l_norm = l.replace("*", "")
                if perito_low in l_norm.lower():
                    after = line.split(":", 1)[1].strip()
                    if after:
                        parts = [p.strip() for p in after.split(",") if p.strip()]
                        out.extend(parts)
    except Exception as e:
        print(f"[AVISO] Falha lendo protocolos em {path}: {e}")
    return out

def _extract_single_row_from_org_table(path: str, perito: str) -> str:
    """Extrai somente a linha do perito de uma tabela Org que tenha a coluna 'Perito'."""
    try:
        with open(path, encoding="utf-8") as f:
            lines = [ln.rstrip("\n") for ln in f]
        i = 0
        while i < len(lines):
            if lines[i].lstrip().startswith("|"):
                tbl = []
                while i < len(lines) and lines[i].lstrip().startswith("|"):
                    tbl.append(lines[i].strip())
                    i += 1
                if not tbl:
                    continue
                header = [h.strip() for h in tbl[0].strip("| ").split("|")]
                if "Perito" not in header:
                    continue
                per_idx = header.index("Perito")
                for row in tbl[2:]:
                    cols = [c.strip() for c in row.strip("| ").split("|")]
                    if per_idx < len(cols) and cols[per_idx].strip().lower() == perito.strip().lower():
                        return "\n".join([
                            "| " + " | ".join(header) + " |",
                            "|" + "|".join("---" for _ in header) + "|",
                            "| " + " | ".join(cols) + " |",
                        ])
            else:
                i += 1
    except Exception as e:
        print(f"[AVISO] Falha extraindo linha da tabela {path}: {e}")
    return ""

# ——— Reincidentes: helpers ————————————————————————————————————————————————
REINC_SCRIPT = os.path.join(SCRIPTS_DIR, "scan_top10_names_with_months.py")

def _run_reincidentes_scan(out_csv: str, min_months: int, root: str,
                           start: str = None, end: str = None,
                           min_analises: int = 50) -> None:
    """
    (NOVA) Gera reincidentes consultando o BANCO, sem depender de .org antigos.
    Jan->end do ano de 'end', mês a mês. Em cada mês, pega o Top 10 (scoreFinal)
    exigindo 'min_analises' tarefas no próprio mês.

    Parâmetros
    ----------
    out_csv : str
        Caminho do CSV de saída (será criado/reescrito).
    min_months : int
        Número mínimo de meses distintos para o perito ser considerado reincidente.
    root : str
        Ignorado nesta versão (mantido por compatibilidade da assinatura).
    start : str | None
        Data inicial YYYY-MM-DD (usada apenas se 'end' vier vazio; caso contrário ignorada).
    end : str | None
        Data final YYYY-MM-DD (define o ano e o último mês da janela Jan->end).
    min_analises : int
        Mínimo de análises no mês para que o perito seja elegível naquele mês.

    Saída
    -----
    CSV com colunas: nome, matricula, CR, DR, meses (lista 'YYYY-MM' separados por vírgula).
    """
    import csv

    # Determina o fim da janela
    if not end:
        if not start:
            print("[AVISO] _run_reincidentes_scan: sem 'end' (nem 'start'). Abortando.")
            return
        end_dt = datetime.strptime(start, "%Y-%m-%d").date()
    else:
        end_dt = datetime.strptime(end, "%Y-%m-%d").date()

    # Jan/AAAA → end
    scan_start = date(end_dt.year, 1, 1)
    scan_end   = end_dt

    # Iterador de meses
    def _iter_months(dini: date, dfim: date):
        y, m = dini.year, dini.month
        while (y < dfim.year) or (y == dfim.year and m <= dfim.month):
            yield y, m
            if m == 12:
                y, m = y + 1, 1
            else:
                m += 1

    conn = sqlite3.connect(DB_PATH)

    # siape -> dados agregados
    seen = {}

    try:
        for y, m in _iter_months(scan_start, scan_end):
            m_ini = date(y, m, 1)
            m_fim = date(y, m, calendar.monthrange(y, m)[1])

            # Top 10 do mês por score, exigindo min_analises no mês
            query = """
                SELECT
                    p.siapePerito AS siape,
                    p.nomePerito  AS nome,
                    p.cr          AS CR,
                    p.dr          AS DR,
                    MAX(i.scoreFinal) AS score,
                    COUNT(a.protocolo) AS total_analises
                FROM indicadores i
                JOIN analises    a ON a.siapePerito = i.perito
                JOIN peritos     p ON p.siapePerito = i.perito
                WHERE date(a.dataHoraIniPericia) BETWEEN ? AND ?
                GROUP BY p.siapePerito, p.nomePerito, p.cr, p.dr
                HAVING total_analises >= ?
                ORDER BY score DESC
                LIMIT 10
            """
            dfm = pd.read_sql(
                query, conn,
                params=(m_ini.isoformat(), m_fim.isoformat(), int(min_analises))
            )
            if dfm.empty:
                continue

            month_key = f"{y:04d}-{m:02d}"
            for _, row in dfm.iterrows():
                siape = str(row["siape"])
                entry = seen.setdefault(siape, {
                    "nome": row["nome"],
                    "CR":   row.get("CR", "") or "",
                    "DR":   row.get("DR", "") or "",
                    "meses": set(),
                })
                entry["meses"].add(month_key)

    finally:
        conn.close()

    # Filtra reincidentes (>= min_months)
    rows = []
    for siape, data in seen.items():
        meses_sorted = sorted(data["meses"])
        if len(meses_sorted) >= (min_months or 1):
            rows.append({
                "nome":      data["nome"],
                "matricula": siape,
                "CR":        data["CR"],
                "DR":        data["DR"],
                "meses":     ", ".join(meses_sorted),
            })

    # Dedup (por matrícula; se matrícula vazia, cai para nome)
    dedup = {}
    for r in rows:
        key = r["matricula"] or r["nome"]
        dedup.setdefault(key, r)

    os.makedirs(os.path.dirname(out_csv), exist_ok=True)
    with open(out_csv, "w", newline="", encoding="utf-8") as f:
        w = csv.DictWriter(f, fieldnames=["nome", "matricula", "CR", "DR", "meses"])
        w.writeheader()
        for r in sorted(dedup.values(), key=lambda x: (x["nome"], x["matricula"])):
            w.writerow(r)

    print(f"✅ Reincidentes (DB, Jan→{scan_end:%Y-%m}, min_analises={min_analises}) salvos em: {out_csv}")

def _csv_to_org_table(csv_path: str, headers=("nome","cr","dr","meses")) -> str:
    """Converte um CSV simples em uma org-table (acesso case-insensitive)."""
    if not os.path.exists(csv_path):
        return ""

    rows = []
    with open(csv_path, newline="", encoding="utf-8") as fh:
        r = csv.DictReader(fh)
        for row in r:
            rows.append(row)
    if not rows:
        return ""

    # Cabeçalhos bonitinhos para a tabela
    hdr_print = []
    for h in headers:
        hl = h.lower()
        if hl == "meses":      hdr_print.append("Meses")
        elif hl == "nome":     hdr_print.append("Perito")
        elif hl == "cr":       hdr_print.append("CR")
        elif hl == "dr":       hdr_print.append("DR")
        elif hl == "matricula":hdr_print.append("Matrícula")
        else:                  hdr_print.append(h)

    lines = []
    lines.append("| " + " | ".join(hdr_print) + " |")
    lines.append("|" + "|".join("---" for _ in hdr_print) + "|")

    # Acesso case-insensitive aos campos
    for row in rows:
        row_ci = { (k or "").strip().lower(): (v or "") for k, v in row.items() }
        vals = [ row_ci.get(h.lower(), "") for h in headers ]
        lines.append("| " + " | ".join(vals) + " |")

    return "\n".join(lines)


def _append_reincidentes_to_org(org_path: str, csv_path: str, min_months: int = 2, heading_level: str = "**"):
    """
    Acrescenta seção de 'Peritos reincidentes' ao final do .org, em fonte pequena
    e com quebras automáticas na coluna 'Meses' (p{largura} + longtable).
    """
    table = _csv_to_org_table(csv_path)
    if not table:
        print("[INFO] Sem reincidentes para anexar (tabela vazia).")
        return False

    section = []
    section.append("")
    section.append(f"{heading_level} Peritos reincidentes no Top 10 (≥ {min_months} meses)")
    section.append("")
    # Atributos LaTeX para a tabela: usa longtable e fixa largura da última coluna
    # Ajuste p{8cm} conforme necessidade (7.5cm, 7cm, etc.)
    section.append("#+ATTR_LATEX: :environment longtable :align l l l p{8cm}")
    # Grupo local para reduzir fonte e padding, sem afetar o resto do documento
    section.append("#+LATEX: \\begingroup\\setlength{\\tabcolsep}{3pt}\\renewcommand{\\arraystretch}{1.0}\\scriptsize")
    section.append(table)
    section.append("#+LATEX: \\endgroup")
    section.append("\n#+LATEX: \\newpage\n")

    with open(org_path, "a", encoding="utf-8") as f:
        f.write("\n".join(section))

    print(f"✅ Reincidentes anexado ao final de: {org_path}")
    return True


# ────────────────────────────────────────────────────────────────────────────────
# Ajustes de Org e utilitários de texto
# ────────────────────────────────────────────────────────────────────────────────
_ANSI_RE     = re.compile(r"\x1b\[[0-9;]*[A-Za-z]")
_BOX_DRAW_RE = re.compile(r"[┌┬┐└┴┘├┼┤│─━┃╭╮╯╰█▓▒░]")

def shift_org_headings(text: str, delta: int = 1) -> str:
    """Rebaixa/eleva níveis de heading em um texto Org."""
    lines = []
    for ln in text.splitlines():
        if ln.startswith('*'):
            i = 0
            while i < len(ln) and ln[i] == '*':
                i += 1
            if i > 0 and i + delta < 7 and (i < len(ln) and ln[i] == ' '):
                ln = ('*' * (i + delta)) + ln[i:]
        lines.append(ln)
    return "\n".join(lines)

def _rewrite_org_image_links(org_text: str, imgs_rel_prefix: str = "../imgs/") -> str:
    """Normaliza links [[file:...]] para apontarem para a pasta imgs/ relativa."""
    def repl(m):
        path = m.group(1).strip()
        if path.startswith(("http://", "https://", "/")):
            return m.group(0)
        base = os.path.basename(path)
        return f"[[file:{imgs_rel_prefix}{base}]]"
    return re.sub(r"\[\[file:([^\]]+)\]\]", repl, org_text)

def _nice_caption(fname: str) -> str:
    """Gera uma legenda amigável a partir do nome do arquivo."""
    base = os.path.splitext(os.path.basename(fname))[0]
    return base.replace("_", " ").replace("-", " ")

def markdown_para_org(texto_md: str) -> str:
    """Converte texto Markdown para Org usando Pandoc, retornando o texto Org."""
    with tempfile.NamedTemporaryFile("w+", suffix=".md", delete=False) as fmd:
        fmd.write(texto_md)
        fmd.flush()
        org_path = fmd.name.replace(".md", ".org")
        subprocess.run(["pandoc", fmd.name, "-t", "org", "-o", org_path])
        with open(org_path, encoding="utf-8") as forg:
            org_text = forg.read()
    return org_text

def _strip_ansi(s: str) -> str:
    return _ANSI_RE.sub("", s)

def _wrap_ascii_blocks(text: str) -> str:
    """Envelopa trechos com pseudografismo/ANSI em #+begin_example/# +end_example."""
    lines = text.splitlines()
    out = []
    in_block = False
    for ln in lines:
        has_ansi = bool(_ANSI_RE.search(ln))
        has_box  = bool(_BOX_DRAW_RE.search(ln))
        if (has_ansi or has_box) and ln.strip():
            if not in_block:
                out.append("#+begin_example")
                in_block = True
            out.append(_strip_ansi(ln))
            continue
        else:
            if in_block:
                out.append("#+end_example")
                in_block = False
            out.append(ln)
    if in_block:
        out.append("#+end_example")
    return "\n".join(out)

def _ensure_blank_lines_around_tables(text: str) -> str:
    """Garante linhas em branco ao redor de tabelas Org para não confundir o Pandoc."""
    text = re.sub(r"(\[Tabela[^\]]*\])\s*(\|)", r"\1\n\2", text, flags=re.I)
    lines = text.splitlines()
    out = []
    i = 0
    while i < len(lines):
        ln = lines[i]
        if ln.lstrip().startswith("|"):
            if out and out[-1].strip() != "":
                out.append("")
            while i < len(lines) and lines[i].lstrip().startswith("|"):
                out.append(lines[i])
                i += 1
            if i < len(lines) and lines[i].strip() != "":
                out.append("")
            continue
        out.append(ln)
        i += 1
    return "\n".join(out)

def _normalize_org_table_block(block_lines):
    """Normaliza um bloco de tabela Org (pipes, espaços e bordas)."""
    fixed = []
    for ln in block_lines:
        raw = ln.strip()
        if raw == "|" or raw == "":
            continue
        raw = re.sub(r"\s*\|\s*", " | ", raw)
        raw = raw.strip()
        if not raw.startswith("|"):
            raw = "| " + raw
        if not raw.endswith("|"):
            raw = raw + " |"
        fixed.append(raw)
    return fixed

def _normalize_all_tables(text: str) -> str:
    """Varre o texto Org e normaliza todas as tabelas."""
    lines = text.splitlines()
    out = []
    i = 0
    while i < len(lines):
        if lines[i].lstrip().startswith("|"):
            block = []
            while i < len(lines) and lines[i].lstrip().startswith("|"):
                block.append(lines[i])
                i += 1
            fixed = _normalize_org_table_block(block)
            if fixed:
                out.extend(fixed)
            if i < len(lines) and lines[i].strip() != "":
                out.append("")
        else:
            out.append(lines[i])
            i += 1
    return "\n".join(out)

def _protect_org_text_for_pandoc(text: str) -> str:
    """Protege e normaliza um texto Org para reduzir erros do Pandoc."""
    text = _ensure_blank_lines_around_tables(text)
    text = _normalize_all_tables(text)
    text = _wrap_ascii_blocks(text)
    return text

def _protect_tables_in_quote(txt: str) -> str:
    """Se encontra tabelas dentro de quotes, envolve com example para não quebrar render."""
    s = re.sub(r'(\S)\n\|', r'\1\n\n|', txt)
    out_lines = []
    in_tbl = False
    for ln in s.splitlines():
        if ln.lstrip().startswith('|'):
            if not in_tbl:
                out_lines.append('#+begin_example')
                in_tbl = True
            out_lines.append(ln)
        else:
            if in_tbl:
                out_lines.append('#+end_example')
                in_tbl = False
            out_lines.append(ln)
    if in_tbl:
        out_lines.append('#+end_example')
    return "\n".join(out_lines)

def _extract_comment_from_org(org_text: str) -> str:
    """
    Extrai comentário de um .org de gráfico:
    1) bloco QUOTE; 2) seção 'Comentário'; 3) 1º parágrafo após a imagem.
    """
    def _strip_drawers(s: str) -> str:
        return re.sub(r'(?ms)^\s*:PROPERTIES:\s*.*?^\s*:END:\s*$', '', s, flags=re.MULTILINE)

    def _strip_noise_lines(s: str) -> str:
        out = []
        for ln in s.splitlines():
            sl = ln.strip()
            if not sl:
                out.append(ln); continue
            if sl.startswith("#+"):   continue
            if sl.startswith("|"):    continue
            if sl.startswith("[[file:"): continue
            out.append(ln)
        return "\n".join(out)

    txt = org_text
    m = re.search(r'(?mis)^\s*#\+BEGIN_QUOTE\s*(.*?)^\s*#\+END_QUOTE', txt)
    if m:
        body = _strip_noise_lines(_strip_drawers(m.group(1))).strip()
        if body:
            return body
    m = re.search(r'(?mis)^\*+\s+coment[aá]ri?o[^\n]*\n(.*?)(?=^\*+\s|\Z)', txt)
    if m:
        body = _strip_noise_lines(_strip_drawers(m.group(1))).strip()
        if body:
            return body
    after_img = re.split(r'(?mi)^\s*\[\[file:[^\]]+\]\]\s*$', txt, maxsplit=1)
    if len(after_img) == 2:
        tail = _strip_noise_lines(_strip_drawers(after_img[1]))
        para = []
        started = False
        for ln in tail.splitlines():
            s = ln.strip()
            if not s:
                if started: break
                else: continue
            if s.startswith("*") or s.startswith("#+") or s.startswith("|"):
                if started: break
                else: continue
            para.append(ln)
            started = True
        res = " ".join(" ".join(para).split())
        if res:
            return res
    return ""

# ────────────────────────────────────────────────────────────────────────────────
# Ordenação/Ranking de imagens para montagem do relatório
# ────────────────────────────────────────────────────────────────────────────────
def _mode_rank(name: str) -> int:
    name = name.lower()
    if "perito-share" in name: return 0
    if "task-share" in name:   return 1
    if "time-share" in name:   return 2
    return 9

def _script_rank(name: str) -> int:
    n = name.lower()
    order = [
        ("nc",           ["nc_rate", "taxa_nc", "nc-rate", "comparenc", "nc_", "rate_nc", "compare_nc_rate", "nao_conformidade"]),
        ("motivos",      ["motivos_perito_vs_brasil", "motivos_top10_vs_brasil", "motivos_", "motivo_"]),
        ("produtiv",     ["produtividade_", "prod_", "productivity_"]),
        ("le15s",        ["le15", "le15s", "compare_15s", "15s"]),
        ("overlap",      ["sobreposicao_", "overlap", "sobrel"]),
        ("composto",     ["indicadores_composto", "composto", "composite"]),
    ]
    for idx, (_, keys) in enumerate(order):
        if any(k in n for k in keys):
            return idx
    return 99

def _png_rank_main(fname: str) -> tuple:
    base = os.path.basename(fname).lower()
    return (_script_rank(base), _mode_rank(base), base)

def _rcheck_perito_rank(fname: str) -> int:
    n = os.path.basename(fname).lower()
    order = [
        "rcheck_nc_rate_", "rcheck_le15", "rcheck_productivity",
        "rcheck_overlap", "rcheck_motivos_chisq", "rcheck_composite",
        "rcheck_weighted_props_nc", "rcheck_weighted_props_le", "rcheck_weighted_props_",
    ]
    for i, key in enumerate(order):
        if key in n:
            return i
    return 99

def _rcheck_group_rank(fname: str) -> int:
    n = os.path.basename(fname).lower()
    order = [
        "rcheck_top10_nc_rate", "rcheck_top10_le15", "rcheck_top10_productivity",
        "rcheck_top10_overlap", "rcheck_top10_motivos_chisq", "rcheck_top10_composite"
    ]
    for i, key in enumerate(order):
        if key in n:
            return i
    if "rcheck_top10_composite_robustness" in n:
        return 5
    return 99


# ────────────────────────────────────────────────────────────────────────────────
# Pré-flight e planners de execução (R e Python)
# ────────────────────────────────────────────────────────────────────────────────
def _preflight_r(r_bin: str):
    """Valida binário do R, mostra versão e encerra se não encontrado."""
    r_path = shutil.which(r_bin)
    if not r_path:
        print(f"❌ Não encontrei o binário '{r_bin}' no PATH. Instale o R e garanta que Rscript esteja disponível.")
        print("   Ex.: sudo apt install r-base-core  (ou ajuste --r-bin com o caminho completo)")
        sys.exit(2)
    try:
        out = subprocess.run([r_bin, "--version"], capture_output=True, text=True)
        ver = (out.stdout or out.stderr or "").splitlines()[0].strip()
        print(f"[R] Ok: {ver} ({r_path})")
    except Exception as e:
        print(f"❌ Falha ao executar '{r_bin} --version': {e}")
        sys.exit(2)

def _is_r_cmd(cmd: list) -> bool:
    """Retorna True se o comando parece ser R/Rscript."""
    if not cmd:
        return False
    exe = os.path.basename(str(cmd[0])).lower()
    return ("rscript" in exe) or exe == "r"

def _detect_r_out_flag(r_file_path: str):
    """Tenta inferir a flag de saída do script R (--out-dir ou --out)."""
    try:
        with open(r_file_path, 'r', encoding='utf-8', errors='ignore') as f:
            txt = f.read().lower()
        if "--out-dir" in txt or "out-dir" in txt or "out_dir" in txt:
            return "--out-dir"
        if "--out" in txt or "make_option(c(\"--out\"" in txt:
            return "--out"
    except Exception:
        pass
    return None

def _r_deps_bootstrap_cmd(r_bin: str):
    """Gera script R para checar/instalar dependências básicas e retorna o comando para rodá-lo."""
    os.makedirs(RCHECK_DIR, exist_ok=True)
    deps_path = os.path.join(RCHECK_DIR, "_ensure_deps.R")
    if not os.path.exists(deps_path):
        r_code = r'''# auto-gerado pelo make_report.py ─ não editar manualmente
options(warn = 1)
repos <- getOption("repos"); repos["CRAN"] <- "https://cloud.r-project.org"; options(repos = repos)
user_lib <- Sys.getenv("R_LIBS_USER")
if (nzchar(user_lib)) { dir.create(user_lib, showWarnings = FALSE, recursive = TRUE); .libPaths(unique(c(user_lib, .libPaths()))) }
message("[deps] .libPaths(): ", paste(.libPaths(), collapse = " | "))
need <- c("dplyr","tidyr","readr","stringr","purrr","forcats","lubridate","ggplot2","scales","broom","DBI","RSQLite","ggtext","gridtext","ragg","textshaping","cli","glue","curl","httr")
have <- rownames(installed.packages()); to_install <- setdiff(need, have)
ncpus <- 1L; try({ ncpus <- max(1L, parallel::detectCores(logical = TRUE) - 1L) }, silent = TRUE)
if (length(to_install)) { message("[deps] Instalando: ", paste(to_install, collapse = ", ")); 
  tryCatch({ install.packages(to_install, dependencies = TRUE, Ncpus = ncpus, lib = if (nzchar(user_lib)) user_lib else .libPaths()[1]) },
           error = function(e) { message("[deps][ERRO] ", conditionMessage(e)); quit(status = 1L) }) 
} else { message("[deps] Todos os pacotes já presentes.") }
ok <- TRUE; for (pkg in c("dplyr","ggplot2","DBI","RSQLite")) ok <- ok && requireNamespace(pkg, quietly = TRUE)
ok <- ok && requireNamespace("ggtext", quietly = TRUE) && requireNamespace("ragg", quietly = TRUE)
if (!ok) message("[deps][AVISO] Verifique dependências de sistema (ex.: libcurl, harfbuzz, fribidi, freetype).") else message("[deps] OK")
message(capture.output(sessionInfo()), sep = "\n")
'''
        with open(deps_path, "w", encoding="utf-8") as f:
            f.write(r_code)
    return [r_bin, deps_path]

def build_commands_for_script(script_file: str, context: dict) -> list:
    """Monta comandos de execução para um script Python de gráfico, dado o contexto (perito/top10)."""
    cmds = []
    name = os.path.basename(script_file)
    help_info = introspect_script(script_file)
    flags = set(help_info.get("flags", set())) or set(ASSUME_FLAGS.get(name, []))
    modes = detect_modes(script_file, help_info)
    base = ["--start", context["start"], "--end", context["end"]]

    if context["kind"] == "perito":
        perito = context["perito"]
        if "--perito" in flags:
            base += ["--perito", perito]
        elif "--nome" in flags:
            base += ["--nome", perito]
        else:
            base += ["--perito", perito]
    else:
        if ("--top10" in flags) or (name in ASSUME_FLAGS and "--top10" in ASSUME_FLAGS[name]):
            base += ["--top10"]
            if "--min-analises" in flags:
                base += ["--min-analises", str(context["min_analises"])]
        else:
            print(f"[INFO] {name} não suporta --top10; pulando no grupo.")
            return []

    if "--export-org" in flags: base += ["--export-org"]
    if context.get("add_comments"):
        if "--export-comment-org" in flags: base += ["--export-comment-org"]
        elif "--export-comment" in flags:   base += ["--export-comment"]
        if "--call-api" in flags:           base += ["--call-api"]
    if "--export-png" in flags: base += ["--export-png"]
    if "--export-md" in flags:  base += ["--export-md"]

    if name == "compare_productivity.py" and "--threshold" in flags:
        base += ["--threshold", PRODUCTIVITY_THRESHOLD]
    if name == "compare_fifteen_seconds.py":
        if "--threshold" in flags: base += ["--threshold", FIFTEEN_THRESHOLD]
        if "--cut-n" in flags:     base += ["--cut-n", FIFTEEN_CUT_N]

    def _apply_extra(cmd_list):
        extra = EXTRA_ARGS.get(name, [])
        i = 0
        while i < len(extra):
            tok = extra[i]
            if isinstance(tok, str) and tok.startswith("--") and tok in flags:
                cmd_list.append(tok)
                if i + 1 < len(extra) and (not isinstance(extra[i+1], str) or not extra[i+1].startswith("--")):
                    cmd_list.append(str(extra[i+1]))
                    i += 1
            i += 1
        return cmd_list

    if "--mode" in flags and modes:
        for m in modes:
            cmd = [sys.executable, script_file] + base + ["--mode", m]
            cmds.append(_apply_extra(cmd))
    else:
        cmd = [sys.executable, script_file] + base
        cmds.append(_apply_extra(cmd))
    return cmds

def build_r_commands_for_perito(perito: str, start: str, end: str, r_bin: str) -> list:
    """Planeja a fila de R checks individuais para o perito."""
    cmds = []
    for fname, meta in RCHECK_SCRIPTS:
        try:
            fpath = rscript_path(fname)
        except FileNotFoundError:
            print(f"[AVISO] R check ausente: {fname} (crie em {RCHECK_DIR})")
            continue
        out_flag = _detect_r_out_flag(fpath)
        cmd = [r_bin, fpath, "--db", DB_PATH, "--start", start, "--end", end]
        if out_flag: cmd += [out_flag, EXPORT_DIR]
        if meta.get("need_perito", False): cmd += ["--perito", perito]
        for k, v in (meta.get("defaults") or {}).items():
            cmd += [k, str(v)]
        cmds.append(cmd)
    if cmds:
        print(f"[INFO] R checks individuais enfileirados para '{perito}': {len(cmds)}")
    else:
        print("[INFO] Nenhum R check individual enfileirado.")
    return cmds

def build_r_commands_for_top10(start: str, end: str, r_bin: str, min_analises: int) -> list:
    """Planeja a fila de R checks para o grupo Top10."""
    cmds = []
    for fname, meta in RCHECK_GROUP_SCRIPTS:
        try:
            fpath = rscript_path(fname)
        except FileNotFoundError:
            print(f"[AVISO] R check de grupo ausente: {fname} (crie em {RCHECK_DIR})")
            continue
        out_flag = _detect_r_out_flag(fpath)
        cmd = [r_bin, fpath, "--db", DB_PATH, "--start", start, "--end", end, "--min-analises", str(min_analises)]
        if meta.get("pass_top10"): cmd += ["--top10"]
        if out_flag: cmd += [out_flag, EXPORT_DIR]
        for k, v in (meta.get("defaults") or {}).items():
            if k == "--min-analises":
                v = str(min_analises)
            cmd += [k, str(v)]
        cmds.append(cmd)
    if cmds:
        print(f"[INFO] R checks de grupo (Top10) enfileirados: {len(cmds)}")
    else:
        print("[INFO] Nenhum R check de grupo enfileirado.")
    return cmds

# ────────────────────────────────────────────────────────────────────────────────
# Coleta de saídas R (fallback)
# ────────────────────────────────────────────────────────────────────────────────
def collect_r_outputs_to_export():
    """Copia rcheck*.{png,md,org} de r_checks/ para exports/ (caso gerados lá)."""
    os.makedirs(EXPORT_DIR, exist_ok=True)
    patterns = ["rcheck*.png", "rcheck*.md", "rcheck*.org", "*top10*.png", "*top10*.md", "*top10*.org"]
    moved = 0
    for pat in patterns:
        for src in glob(os.path.join(RCHECK_DIR, pat)):
            dst = os.path.join(EXPORT_DIR, os.path.basename(src))
            try:
                shutil.copy2(src, dst)
                moved += 1
            except Exception:
                pass
    if moved:
        print(f"[INFO] R-outputs coletados de r_checks/ → exports/: {moved} arquivo(s).")

# ────────────────────────────────────────────────────────────────────────────────
# Limpeza, cópia e organização de artefatos (.png, .org, .md)
# ────────────────────────────────────────────────────────────────────────────────
def _cleanup_exports_for_perito(safe_perito: str):
    """Remove do exports/ os artefatos pendentes do perito (para evitar mistura entre execuções)."""
    for f in glob(os.path.join(EXPORT_DIR, f"*_{safe_perito}.*")):
        try: os.remove(f)
        except Exception: pass
    for f in glob(os.path.join(EXPORT_DIR, f"*{safe_perito}.*")):
        if "top10" in os.path.basename(f).lower():
            continue
        try: os.remove(f)
        except Exception: pass

def _cleanup_exports_top10():
    """Remove do exports/ os artefatos pendentes de top10."""
    for f in glob(os.path.join(EXPORT_DIR, "*_top10*.*")):
        try: os.remove(f)
        except Exception: pass
    for f in glob(os.path.join(EXPORT_DIR, "*top10*.*")):
        try: os.remove(f)
        except Exception: pass

def _move_md_generic_to(markdown_dir: str, pattern: str = "*.md"):
    """Move quaisquer .md remanescentes de exports/ para a pasta markdown/."""
    os.makedirs(markdown_dir, exist_ok=True)
    for src in glob(os.path.join(EXPORT_DIR, pattern)):
        base = os.path.basename(src)
        dst  = os.path.join(markdown_dir, base)
        try:
            shutil.copy2(src, dst)
            os.remove(src)
        except Exception:
            pass

def copiar_artefatos_perito(perito: str, imgs_dir: str, comments_dir: str, orgs_dir: str = None, markdown_dir: str = None):
    """
    Copia artefatos do perito de exports/ para as pastas do relatório:
    - imgs/      (PNGs)
    - comments/  (comentários .org e conversões de .md→.org)
    - orgs/      (.org auxiliares dos scripts, com links normalizados)
    - markdown/  (NOVO: todos os .md são preservados aqui)
    """
    if orgs_dir is None:
        orgs_dir = os.path.join(os.path.dirname(imgs_dir), "orgs")
    if markdown_dir is None:
        markdown_dir = os.path.join(os.path.dirname(imgs_dir), "markdown")

    os.makedirs(imgs_dir, exist_ok=True)
    os.makedirs(comments_dir, exist_ok=True)
    os.makedirs(orgs_dir, exist_ok=True)
    os.makedirs(markdown_dir, exist_ok=True)

    safe = _safe(perito)

    # PNGs
    for pat in (f"*_{safe}.png", f"*{safe}.png"):
        for src in glob(os.path.join(EXPORT_DIR, pat)):
            base = os.path.basename(src)
            if "top10" in base.lower():
                continue
            shutil.copy2(src, os.path.join(imgs_dir, base))
            try: os.remove(src)
            except Exception: pass

    # Comentários já em .org
    for pat in (f"*_{safe}_comment.org", f"*{safe}_comment.org"):
        for src in glob(os.path.join(EXPORT_DIR, pat)):
            base = os.path.basename(src)
            if "top10" in base.lower():
                continue
            with open(src, encoding="utf-8") as f:
                content = f.read()
            dst = os.path.join(comments_dir, base)
            with open(dst, "w", encoding="utf-8") as g:
                g.write(content)
            try: os.remove(src)
            except Exception: pass

    # Comentários em .md → converte para .org (para o relatório) e também MOVE o .md para markdown/
    for pat in (f"*_{safe}_comment.md", f"*{safe}_comment.md", f"*_{safe}.md", f"*{safe}.md"):
        for src in glob(os.path.join(EXPORT_DIR, pat)):
            base = os.path.basename(src)
            if "top10" in base.lower():
                continue
            # 1) converter .md -> .org para uso no relatório
            try:
                with open(src, encoding="utf-8") as f:
                    md_text = f.read()
                org_text = markdown_para_org(md_text)
                org_text = "\n".join(ln for ln in org_text.splitlines() if not ln.strip().lower().startswith('#+title')).strip()
                dst_org = os.path.join(comments_dir, os.path.splitext(base)[0] + ".org")
                with open(dst_org, "w", encoding="utf-8") as g:
                    g.write(org_text + "\n")
            except Exception as e:
                print(f"[AVISO] Falha na conversão .md→.org de {base}: {e}")

            # 2) preservar o .md em markdown/
            try:
                shutil.copy2(src, os.path.join(markdown_dir, base))
                os.remove(src)
            except Exception:
                pass

    # ORGs auxiliares dos scripts (links normalizados para ../imgs/)
    for pat in (f"*_{safe}.org", f"*{safe}.org"):
        for src in glob(os.path.join(EXPORT_DIR, pat)):
            base = os.path.basename(src)
            if "top10" in base.lower():
                continue
            with open(src, encoding="utf-8") as f:
                content = f.read()
            content = _rewrite_org_image_links(content, imgs_rel_prefix="../imgs/")
            dst = os.path.join(orgs_dir, base)
            with open(dst, "w", encoding="utf-8") as g:
                g.write(content)
            try: os.remove(src)
            except Exception: pass

    # Por fim, move qualquer outro .md remanescente
    _move_md_generic_to(markdown_dir)

def copiar_artefatos_top10(imgs_dir: str, comments_dir: str, orgs_dir: str = None, markdown_dir: str = None):
    """
    Copia artefatos do grupo top10:
    - imgs/      (PNGs)
    - comments/  (comentários .org e conversões de .md→.org)
    - orgs/      (.org auxiliares dos scripts, com links normalizados)
    - markdown/  (NOVO: todos os .md são preservados aqui)
    """
    if orgs_dir is None:
        orgs_dir = os.path.join(os.path.dirname(imgs_dir), "orgs")
    if markdown_dir is None:
        markdown_dir = os.path.join(os.path.dirname(imgs_dir), "markdown")

    os.makedirs(imgs_dir, exist_ok=True)
    os.makedirs(comments_dir, exist_ok=True)
    os.makedirs(orgs_dir, exist_ok=True)
    os.makedirs(markdown_dir, exist_ok=True)

    # PNGs
    for pat in ("*_top10*.png", "*top10*.png"):
        for src in glob(os.path.join(EXPORT_DIR, pat)):
            base = os.path.basename(src)
            shutil.copy2(src, os.path.join(imgs_dir, base))
            try: os.remove(src)
            except Exception: pass

    # Comentários .org (grupo)
    for pat in ("*_top10*_comment.org", "*top10*_comment.org"):
        for src in glob(os.path.join(EXPORT_DIR, pat)):
            base = os.path.basename(src)
            shutil.copy2(src, os.path.join(comments_dir, base))
            try: os.remove(src)
            except Exception: pass

    # Comentários .md → converte p/ .org e preserva .md em markdown/
    for pat in ("*_top10*_comment.md", "*top10*_comment.md", "*_top10*.md", "*top10*.md"):
        for src in glob(os.path.join(EXPORT_DIR, pat)):
            base = os.path.basename(src)
            try:
                with open(src, encoding="utf-8") as f:
                    md_text = f.read()
                org_text = markdown_para_org(md_text)
                org_text = "\n".join(ln for ln in org_text.splitlines() if not ln.strip().lower().startswith('#+title')).strip()
                dst_org = os.path.join(comments_dir, os.path.splitext(base)[0] + ".org")
                with open(dst_org, "w", encoding="utf-8") as g:
                    g.write(org_text + "\n")
            except Exception as e:
                print(f"[AVISO] Falha na conversão .md→.org de {base}: {e}")
            try:
                shutil.copy2(src, os.path.join(markdown_dir, base))
                os.remove(src)
            except Exception:
                pass

    # ORGs auxiliares (links normalizados)
    for pat in ("*_top10*.org", "*top10*.org"):
        for src in glob(os.path.join(EXPORT_DIR, pat)):
            base = os.path.basename(src)
            with open(src, encoding="utf-8") as f:
                content = f.read()
            content = _rewrite_org_image_links(content, imgs_rel_prefix="../imgs/")
            dst = os.path.join(orgs_dir, base)
            with open(dst, "w", encoding="utf-8") as g:
                g.write(content)
            try: os.remove(src)
            except Exception: pass

    # Move quaisquer .md restantes
    _move_md_generic_to(markdown_dir)

def copiar_artefatos_weekday2weekend(imgs_dir: str, comments_dir: str, orgs_dir: str = None):
    """
    Move o panorama weekday→weekend:
      - PNG: rcheck_weekday_to_weekend_by_cr.png → imgs/
      - .org: rcheck_weekday_to_weekend_table.org, rcheck_weekday_to_weekend_protocols.org → orgs/
      - .org de comentário → comments/
    """
    if orgs_dir is None:
        orgs_dir = os.path.join(os.path.dirname(imgs_dir), "orgs")
    os.makedirs(imgs_dir, exist_ok=True)
    os.makedirs(comments_dir, exist_ok=True)
    os.makedirs(orgs_dir, exist_ok=True)

    for fname in ("rcheck_weekday_to_weekend_by_cr.png",):
        src = os.path.join(EXPORT_DIR, fname)
        dst = os.path.join(imgs_dir, fname)
        if os.path.exists(src):
            shutil.copy2(src, dst)
            try: os.remove(src)
            except Exception: pass

    for fname in ("rcheck_weekday_to_weekend_table.org", "rcheck_weekday_to_weekend_protocols.org"):
        src = os.path.join(EXPORT_DIR, fname)
        dst = os.path.join(orgs_dir, fname)
        if os.path.exists(src):
            try:
                with open(src, encoding="utf-8") as f:
                    content = f.read()
                content = _rewrite_org_image_links(content, imgs_rel_prefix="../imgs/")
                with open(dst, "w", encoding="utf-8") as g:
                    g.write(content)
            except Exception as e:
                print(f"[AVISO] Falha ao normalizar {fname}: {e}. Copiando bruto.")
                shutil.copy2(src, dst)
            finally:
                try: os.remove(src)
                except Exception: pass

    for fname in ("rcheck_weekday_to_weekend_table_comment.org",):
        src = os.path.join(EXPORT_DIR, fname)
        dst = os.path.join(comments_dir, fname)
        if os.path.exists(src):
            shutil.copy2(src, dst)
            try: os.remove(src)
            except Exception: pass

# ────────────────────────────────────────────────────────────────────────────────
# Estatísticas rápidas (cabeçalho)
# ────────────────────────────────────────────────────────────────────────────────
def get_summary_stats(perito, start, end):
    """Retorna (total_tarefas, pct_nc, CR, DR) do perito no período."""
    conn = sqlite3.connect(DB_PATH)
    cur = conn.cursor()
    cur.execute("""SELECT cr, dr FROM peritos WHERE nomePerito = ?""", (perito,))
    row = cur.fetchone()
    cr, dr = (row if row else ("-", "-"))
    cur.execute("""
        SELECT
            COUNT(*) AS total,
            SUM(
                CASE
                    WHEN CAST(IFNULL(a.conformado, 1) AS INTEGER) = 0 THEN 1
                    WHEN TRIM(IFNULL(a.motivoNaoConformado, '')) <> ''
                         AND CAST(IFNULL(a.motivoNaoConformado, '0') AS INTEGER) <> 0 THEN 1
                    ELSE 0
                END
            ) AS nc_count
        FROM analises a
        JOIN peritos p ON a.siapePerito = p.siapePerito
        WHERE p.nomePerito = ?
          AND date(a.dataHoraIniPericia) BETWEEN ? AND ?
    """, (perito, start, end))
    total, nc_count = cur.fetchone() or (0, 0)
    conn.close()
    pct_nc = (nc_count or 0) / (total or 1) * 100.0
    return total or 0, pct_nc, cr, dr

# ────────────────────────────────────────────────────────────────────────────────
# Apêndices por perito
# ────────────────────────────────────────────────────────────────────────────────
def gerar_apendice_nc(perito, start, end):
    """Retorna DataFrame com protocolos NC por motivo para o perito no período."""
    conn = sqlite3.connect(DB_PATH)
    df = pd.read_sql("""
        SELECT a.protocolo, pr.motivo AS motivo_text
        FROM analises a
        JOIN peritos p ON a.siapePerito = p.siapePerito
        JOIN protocolos pr ON a.protocolo = pr.protocolo
        WHERE p.nomePerito = ?
          AND date(a.dataHoraIniPericia) BETWEEN ? AND ?
          AND (
                CAST(IFNULL(a.conformado, 1) AS INTEGER) = 0
                OR (
                    TRIM(IFNULL(a.motivoNaoConformado, '')) <> ''
                    AND CAST(IFNULL(a.motivoNaoConformado, '0') AS INTEGER) <> 0
                )
              )
        ORDER BY a.protocolo
    """, conn, params=(perito, start, end))
    conn.close()
    return df

def gerar_r_apendice_comments_if_possible(perito: str, imgs_dir: str, comments_dir: str, start: str, end: str):
    """Gera comentários GPT para R checks individuais (se utils.comentarios estiver disponível)."""
    if comentar_r_apendice is None:
        return
    safe = _safe(perito)
    r_pngs = glob(os.path.join(imgs_dir, f"rcheck_*_{safe}.png"))
    r_pngs.sort(key=lambda p: _rcheck_perito_rank(p))
    for png in r_pngs:
        base = os.path.basename(png)
        stem = os.path.splitext(base)[0]
        md_out = os.path.join(comments_dir, f"{stem}_comment.md")
        try:
            texto = comentar_r_apendice(
                titulo="Apêndice estatístico (R) — Perito",
                imagem_rel=f"imgs/{base}",
                perito=perito,
                start=start,
                end=end
            )
            with open(md_out, "w", encoding="utf-8") as f:
                f.write(texto)
        except Exception as e:
            print(f"[AVISO] Falha ao gerar comentário GPT do R check {base}: {e}")

def gerar_r_apendice_group_comments_if_possible(imgs_dir: str, comments_dir: str, start: str, end: str):
    """Gera comentários GPT para R checks do grupo Top10 (se disponível)."""
    if comentar_r_apendice is None:
        return
    r_pngs = glob(os.path.join(imgs_dir, "rcheck_top10_*.png"))
    r_pngs.sort(key=lambda p: _rcheck_group_rank(p))
    for png in r_pngs:
        base = os.path.basename(png)
        stem = os.path.splitext(base)[0]
        md_out = os.path.join(comments_dir, f"{stem}_comment.md")
        try:
            texto = comentar_r_apendice(
                titulo="Apêndice estatístico (R) — Top 10 (grupo)",
                imagem_rel=f"imgs/{base}",
                perito=None,
                start=start,
                end=end
            )
            with open(md_out, "w", encoding="utf-8") as f:
                f.write(texto)
        except Exception as e:
            print(f"[AVISO] Falha ao gerar comentário GPT do R check de grupo {base}: {e}")

# ────────────────────────────────────────────────────────────────────────────────
# Montagem dos .org (perito e grupo)
# ────────────────────────────────────────────────────────────────────────────────
def gerar_org_perito(perito, start, end, add_comments, imgs_dir, comments_dir, output_dir):
    """
    Monta o arquivo {perito}.org com:
      - cabeçalho de métricas
      - gráficos (ordenados)
      - comentários (quando existirem)
      - apêndice de protocolos NC
      - apêndice de R checks
    """
    safe = _safe(perito)
    org_path = os.path.join(output_dir, f"{safe}.org")

    lines = [f"** {perito}"]
    total, pct_nc, cr, dr = get_summary_stats(perito, start, end)
    lines += [f"- Tarefas: {total}", f"- % NC: {pct_nc:.1f}", f"- CR: {cr} | DR: {dr}", ""]

    # Gráficos principais (Python) — ORDEM CONTROLADA
    all_pngs = glob(os.path.join(imgs_dir, f"*{safe}.png"))
    main_pngs = [p for p in all_pngs if not os.path.basename(p).lower().startswith("rcheck_")]
    main_pngs.sort(key=_png_rank_main)

    for png in main_pngs:
        base = os.path.basename(png)
        lines += [
            "#+ATTR_LATEX: :placement [H] :width \\linewidth",
            f"#+CAPTION: {_nice_caption(base)}",
            f"[[file:imgs/{base}]]",
        ]
        if add_comments:
            stem = os.path.splitext(base)[0]
            quote_lines = _inject_comment_for_stem(stem, comments_dir, output_dir, imgs_dir=imgs_dir)
            lines += quote_lines

        lines.append("\n#+LATEX: \\newpage\n")

    # Apêndice: Protocolos NC por motivo
    apdf = gerar_apendice_nc(perito, start, end)
    if not apdf.empty:
        lines.append(f"*** Apêndice: Protocolos Não-Conformados por Motivo")
        grouped = apdf.groupby('motivo_text')['protocolo'].apply(lambda seq: ', '.join(map(str, seq))).reset_index()
        for _, grp in grouped.iterrows():
            lines.append(f"- *{grp['motivo_text']}*: {grp['protocolo']}")
        lines.append("")

    # Apêndice: R checks (perito) — ORDEM CONTROLADA
    r_pngs = glob(os.path.join(imgs_dir, f"rcheck_*_{safe}.png"))
    r_pngs.sort(key=lambda p: _rcheck_perito_rank(p))
    if r_pngs:
        lines.append(f"*** Apêndice estatístico (R) — {perito}\n")
        for png in r_pngs:
            base = os.path.basename(png)
            lines += [
                "#+ATTR_LATEX: :placement [H] :width \\linewidth",
                f"#+CAPTION: {_nice_caption(base)}",
                f"[[file:imgs/{base}]]",
            ]
            if add_comments:
                stem = os.path.splitext(base)[0]
                quote_lines = _inject_comment_for_stem(stem, comments_dir, output_dir, imgs_dir=imgs_dir)
                lines += quote_lines
            lines.append("\n#+LATEX: \\newpage\n")

    with open(org_path, "w", encoding="utf-8") as f:
        f.write("\n".join(lines))
    print(f"✅ Org individual salvo em: {org_path}")
    return org_path


def gerar_org_top10_grupo(start, end, output_dir, imgs_dir, comments_dir):
    """
    Monta o org do grupo Top10 (gráficos + comentários, incluindo R checks do grupo).
    """
    org_path = os.path.join(output_dir, f"top10_grupo.org")
    lines = [f"** Top 10 — Gráficos do Grupo ({start} a {end})\n"]

    all_pngs = sorted(glob(os.path.join(imgs_dir, "*_top10*.png")) + glob(os.path.join(imgs_dir, "*top10*.png")))
    main_pngs = [p for p in all_pngs if "rcheck_" not in os.path.basename(p).lower()]
    r_pngs    = [p for p in all_pngs if "rcheck_" in os.path.basename(p).lower()]

    main_pngs.sort(key=_png_rank_main)
    r_pngs.sort(key=lambda p: _rcheck_group_rank(p))

    for png in main_pngs + r_pngs:
        base = os.path.basename(png)
        lines += [
            "#+ATTR_LATEX: :placement [H] :width \\linewidth",
            f"#+CAPTION: {_nice_caption(base)}",
            f"[[file:imgs/{base}]]",
        ]
        stem = os.path.splitext(base)[0]
        quote_lines = _inject_comment_for_stem(stem, comments_dir, output_dir)
        lines += quote_lines

        lines.append("\n#+LATEX: \\newpage\n")

    with open(org_path, "w", encoding="utf-8") as f:
        f.write("\n".join(lines))
    print(f"✅ Org do grupo Top 10 salvo em: {org_path}")
    return org_path


def gerar_org_individual_consolidado(perito, start, end, relatorio_dir):
    """Envolve o {perito}.org em um container 'Relatório individual — ...'."""
    safe = _safe(perito)
    perito_org = os.path.join(relatorio_dir, f"{safe}.org")
    final_org  = os.path.join(relatorio_dir, f"relatorio_{safe}_{start}_a_{end}.org")
    if not os.path.exists(perito_org):
        raise FileNotFoundError(f"Org individual não encontrado: {perito_org}")
    with open(perito_org, "r", encoding="utf-8") as f:
        content = f.read().strip()
    content = _protect_org_text_for_pandoc(content)
    lines = [f"* Relatório individual — {perito} ({start} a {end})", "", content, ""]
    with open(final_org, "w", encoding="utf-8") as g:
        g.write("\n".join(lines))
    print(f"✅ Org consolidado (individual) salvo em: {final_org}")
    return final_org

def gerar_org_individual_consolidado_com_panorama(perito, start, end, relatorio_dir):
    """(Opcional) Insere panorama global UMA VEZ antes da seção do perito."""
    safe = _safe(perito)
    perito_org = os.path.join(relatorio_dir, f"{safe}.org")
    final_org  = os.path.join(relatorio_dir, f"relatorio_{safe}_{start}_a_{end}.org")
    if not os.path.exists(perito_org):
        raise FileNotFoundError(f"Org individual não encontrado: {perito_org}")
    with open(perito_org, "r", encoding="utf-8") as f:
        content = f.read().strip()
    content = _protect_org_text_for_pandoc(content)
    lines = []
    _append_weekday2weekend_panorama_block(lines, os.path.join(relatorio_dir, "imgs"), os.path.join(relatorio_dir, "comments"), heading_level="**")
    lines.extend([content, ""])
    with open(final_org, "w", encoding="utf-8") as g:
        g.write("\n".join(lines))
    print(f"✅ Org consolidado (individual) salvo em: {final_org}")
    return final_org

def gerar_org_individual_consolidado_com_panorama_depois(perito: str, start: str, end: str, relatorio_dir: str, perito_org_path: str) -> str:
    """Gera .org individual e coloca o panorama weekday→weekend AO FINAL (se existir)."""
    imgs_dir     = os.path.join(relatorio_dir, "imgs")
    comments_dir = os.path.join(relatorio_dir, "comments")
    safe_perito  = _safe(perito)
    org_final    = os.path.join(relatorio_dir, f"relatorio_{safe_perito}_{start}_a_{end}.org")

    lines = [f"* Relatório individual — {perito} ({start} a {end})", ""]
    if perito_org_path and os.path.exists(perito_org_path):
        with open(perito_org_path, encoding="utf-8") as f:
            content = f.read().strip()
        if content:
            lines.append(content)
            lines.append("#+LATEX: \\newpage\n")
    else:
        print(f"[AVISO] Org do perito não encontrado: {perito_org_path}")

    _append_weekday2weekend_panorama_block(lines, imgs_dir, comments_dir, start=start, end=end, heading_level="**")

    with open(org_final, "w", encoding="utf-8") as f:
        f.write("\n".join(lines).strip() + "\n")
    print(f"✅ Org consolidado (individual) salvo em: {org_final}")
    return org_final

# ────────────────────────────────────────────────────────────────────────────────
# Seleção Top 10 & Coorte Extra (funções auxiliares usadas no main)
# ────────────────────────────────────────────────────────────────────────────────

def pegar_10_piores_peritos(start: str, end: str, min_analises: int = 50) -> pd.DataFrame:
    """
    Retorna os 10 piores peritos no período (start..end) de acordo com 'scoreFinal',
    considerando apenas peritos com pelo menos 'min_analises' tarefas.

    Tabelas esperadas:
      - indicadores (coluna: perito = siapePerito, scoreFinal)
      - peritos     (colunas: siapePerito, nomePerito)
      - analises    (colunas: siapePerito, protocolo, dataHoraIniPericia)

    Obs.: 'scoreFinal' maior = pior (ordena DESC).
    """
    conn = sqlite3.connect(DB_PATH)
    try:
        query = """
        SELECT
            p.nomePerito,
            i.scoreFinal,
            COUNT(a.protocolo) AS total_analises
        FROM indicadores i
        JOIN peritos     p ON i.perito      = p.siapePerito
        JOIN analises    a ON a.siapePerito = i.perito
        WHERE date(a.dataHoraIniPericia) BETWEEN ? AND ?
        GROUP BY p.nomePerito, i.scoreFinal
        HAVING total_analises >= ?
        ORDER BY i.scoreFinal DESC
        LIMIT 10
        """
        df = pd.read_sql(query, conn, params=(start, end, min_analises))
        return df
    finally:
        conn.close()


def pegar_peritos_nc_altissima(start: str, end: str,
                               nc_threshold: float = 90.0,
                               min_tasks: int = 50) -> pd.DataFrame:
    """
    Retorna peritos com % de não conformidade >= nc_threshold e total de tarefas >= min_tasks
    no período (start..end). A %NC é calculada por perito sobre suas próprias tarefas.
    """
    conn = sqlite3.connect(DB_PATH)
    try:
        query = """
            SELECT
                p.nomePerito,
                COUNT(*) AS total,
                SUM(
                    CASE
                        WHEN CAST(IFNULL(a.conformado, 1) AS INTEGER) = 0 THEN 1
                        WHEN TRIM(IFNULL(a.motivoNaoConformado, '')) <> ''
                             AND CAST(IFNULL(a.motivoNaoConformado, '0') AS INTEGER) <> 0 THEN 1
                        ELSE 0
                    END
                ) AS nc_count,
                (SUM(
                    CASE
                        WHEN CAST(IFNULL(a.conformado, 1) AS INTEGER) = 0 THEN 1
                        WHEN TRIM(IFNULL(a.motivoNaoConformado, '')) <> ''
                             AND CAST(IFNULL(a.motivoNaoConformado, '0') AS INTEGER) <> 0 THEN 1
                    END
                ) * 100.0) / COUNT(*) AS pct_nc
            FROM analises a
            JOIN peritos p ON a.siapePerito = p.siapePerito
            WHERE date(a.dataHoraIniPericia) BETWEEN ? AND ?
            GROUP BY p.nomePerito
            HAVING total >= ? AND pct_nc >= ?
            ORDER BY pct_nc DESC, total DESC
        """
        df = pd.read_sql(query, conn, params=(start, end, min_tasks, nc_threshold))
        return df
    finally:
        conn.close()


def perito_tem_dados(perito: str, start: str, end: str) -> bool:
    """
    True se o perito possui pelo menos 1 tarefa no período.
    """
    conn = sqlite3.connect(DB_PATH)
    try:
        count = conn.execute(
            """
            SELECT COUNT(*)
            FROM analises a
            JOIN peritos p ON a.siapePerito = p.siapePerito
            WHERE p.nomePerito = ?
              AND date(a.dataHoraIniPericia) BETWEEN ? AND ?
            """,
            (perito, start, end)
        ).fetchone()[0]
        return count > 0
    finally:
        conn.close()

# ────────────────────────────────────────────────────────────────────────────────
# Exportação para PDF e main()
# ────────────────────────────────────────────────────────────────────────────────

LATEX_HEADER_CONTENT = r"""
%% Inserido automaticamente pelo make_report.py
\usepackage{float}
\usepackage{placeins}
\floatplacement{figure}{H}

% Pacotes úteis para tabelas compridas e colunas com largura fixa
\usepackage{longtable}
\usepackage{array}
"""


def exportar_org_para_pdf(org_path: str, font: str = "DejaVu Sans") -> str | None:
    """
    Converte um arquivo .org para PDF via Pandoc + xelatex, aplicando um header LaTeX
    para 'segurar' as figuras (placement [H]) e garantindo margens.

    Parâmetros
    ----------
    org_path : str
        Caminho absoluto para o arquivo .org a ser exportado.
    font : str, opcional
        Fonte principal do PDF (default: 'DejaVu Sans').

    Retorna
    -------
    str | None
        Caminho do PDF gerado, ou None em caso de falha (mensagem de log é impressa).
    """
    import shutil as sh

    output_dir = os.path.dirname(org_path)
    org_name   = os.path.basename(org_path)
    pdf_name   = org_name.replace('.org', '.pdf')
    log_path   = org_path + ".log"
    header_path = os.path.join(output_dir, "_header_figs.tex")

    # Escreve header LaTeX
    with open(header_path, "w", encoding="utf-8") as fh:
        fh.write(LATEX_HEADER_CONTENT)

    # Protege o .org para consumo do Pandoc (tabelas, ASCII-art, etc.)
    with open(org_path, "r", encoding="utf-8") as f:
        raw = f.read()
    protected = _protect_org_text_for_pandoc(raw)
    prot_name = org_name.replace(".org", "._pandoc.org")
    prot_path = os.path.join(output_dir, prot_name)
    with open(prot_path, "w", encoding="utf-8") as fprot:
        fprot.write(protected)

    # Checa Pandoc
    pandoc = sh.which("pandoc")
    if not pandoc:
        print("❌ Pandoc não encontrado no PATH. Instale com: sudo apt install pandoc texlive-xetex")
        return None

    # Monta comando Pandoc
    cmd = [
        "pandoc", prot_name, "-o", pdf_name,
        "--pdf-engine=xelatex",
        "--include-in-header", os.path.basename(header_path),
        "--variable", f"mainfont={font}",
        "--variable", "geometry:margin=2cm",
        "--highlight-style=zenburn",
    ]

    print(f"[Pandoc] Gerando PDF: {' '.join(cmd)} (cwd={output_dir})")
    prev_cwd = os.getcwd()
    try:
        os.chdir(output_dir)
        with open(log_path, "w", encoding="utf-8") as flog:
            result = subprocess.run(cmd, stdout=flog, stderr=flog, text=True)
    finally:
        os.chdir(prev_cwd)

    pdf_path = os.path.join(output_dir, pdf_name)
    if result.returncode == 0 and os.path.exists(pdf_path):
        print(f"✅ PDF gerado: {pdf_path}")
        return pdf_path
    else:
        print(f"❌ Erro ao gerar PDF. Veja o log: {log_path}")
        return None


def adicionar_capa_pdf(pdf_final_path: str) -> None:
    """
    Prependa a capa oficial (misc/capa.pdf) ao PDF final.
    Gera um novo arquivo com sufixo '_com_capa.pdf' no mesmo diretório.

    Parâmetros
    ----------
    pdf_final_path : str
        Caminho do PDF base (sem capa).
    """
    capa_path = os.path.join(MISC_DIR, "capa.pdf")
    if not os.path.exists(capa_path):
        print(f"[AVISO] Capa não encontrada: {capa_path}. Pulando.")
        return
    if not os.path.exists(pdf_final_path):
        print(f"[ERRO] PDF base não encontrado: {pdf_final_path}.")
        return

    output_path = pdf_final_path.replace(".pdf", "_com_capa.pdf")
    merger = PdfMerger()
    try:
        merger.append(capa_path)
        merger.append(pdf_final_path)
        merger.write(output_path)
        merger.close()
        print(f"✅ Relatório final com capa: {output_path}")
    except Exception as e:
        print(f"[ERRO] Falha ao adicionar capa: {e}")

def _mover_markdowns_de_exports(markdown_dir: str) -> int:
    """
    Move todos os arquivos '.md' que restarem em EXPORT_DIR para a pasta 'markdown/'
    do relatório corrente. Isso ocorre após:
      - conversão de comentários .md → .org (que já remove os .md correspondentes); e
      - cópia dos artefatos principais (.png/.org) para suas pastas.

    Parâmetros
    ----------
    markdown_dir : str
        Caminho para a pasta 'markdown' do relatório (será criada se necessário).

    Retorna
    -------
    int
        Quantidade de arquivos .md movidos.
    """
    os.makedirs(markdown_dir, exist_ok=True)
    moved = 0
    for src in glob(os.path.join(EXPORT_DIR, "*.md")):
        dst = os.path.join(markdown_dir, os.path.basename(src))
        try:
            shutil.copy2(src, dst)
            os.remove(src)
            moved += 1
        except Exception as e:
            print(f"[AVISO] Falha movendo {src} → {dst}: {e}")
    if moved:
        print(f"[INFO] Markdowns movidos para '{markdown_dir}': {moved} arquivo(s).")
    return moved

def main():
    """
    Orquestração principal:
      1) Parse de argumentos e preflight (env, R).
      2) Planejamento e execução dos gráficos (Python e R).
      3) Coleta e cópia dos artefatos para pastas do relatório.
      4) Geração do(s) arquivo(s) .org consolidado(s) e, opcionalmente, PDF com capa.
      5) Organização dos arquivos '.md' remanescentes em 'markdown/'.
    """
    t0 = time.time()
    args = parse_args()

    # Aviso apenas se pretendemos gerar comentários IA sem OPENAI_API_KEY
    if (args.add_comments or getattr(args, "export_comment_org", False)) and not os.getenv("OPENAI_API_KEY"):
        print("⚠️  Sem OPENAI_API_KEY. Os scripts com comentário podem cair no fallback (sem IA).")

    # Preflight do R, se apêndice habilitado
    if args.r_appendix:
        _preflight_r(args.r_bin)

    # Estrutura de diretórios por período e tipo (Top10 ou Individual)
    PERIODO_DIR = os.path.join(OUTPUTS_DIR, f"{args.start}_a_{args.end}")
    if args.top10:
        RELATORIO_DIR = os.path.join(PERIODO_DIR, "top10")
    else:
        safe_perito = _safe(args.perito.strip())
        RELATORIO_DIR = os.path.join(PERIODO_DIR, "individual", safe_perito)

    IMGS_DIR      = os.path.join(RELATORIO_DIR, "imgs")
    COMMENTS_DIR  = os.path.join(RELATORIO_DIR, "comments")
    ORGS_DIR      = os.path.join(RELATORIO_DIR, "orgs")
    MARKDOWN_DIR  = os.path.join(RELATORIO_DIR, "markdown")

    for d in (PERIODO_DIR, RELATORIO_DIR, IMGS_DIR, COMMENTS_DIR, ORGS_DIR, MARKDOWN_DIR):
        os.makedirs(d, exist_ok=True)

    planned_cmds = []

    # =========================
    # PLANEJAMENTO — TOP 10
    # =========================
    if args.top10:
        peritos_df = pegar_10_piores_peritos(args.start, args.end, min_analises=args.min_analises)
        if peritos_df.empty:
            print("Nenhum perito encontrado com os critérios.")
            return

        lista_top10 = peritos_df['nomePerito'].tolist()
        set_top10   = set(lista_top10)
        print(f"Gerando para os 10 piores: {lista_top10}")

        # Limpa exports "top10" antigos
        _cleanup_exports_top10()

        # Contexto de grupo
        group_ctx = {
            "kind": "top10",
            "start": args.start,
            "end":   args.end,
            "min_analises": args.min_analises,
            "add_comments": args.add_comments,
        }

        # Agenda scripts de grupo (Python)
        for script in SCRIPT_ORDER:
            script_file = script_path(script)
            planned_cmds.extend(build_commands_for_script(script_file, group_ctx))

        # Agenda R checks do grupo
        if args.r_appendix:
            planned_cmds.extend(build_r_commands_for_top10(args.start, args.end, args.r_bin, args.min_analises))

        # Agenda scripts individuais (para cada perito do Top10)
        for perito in lista_top10:
            if not perito_tem_dados(perito, args.start, args.end):
                print(f"⚠️  Perito '{perito}' sem análises no período! Pulando.")
                continue

            safe = _safe(perito)
            _cleanup_exports_for_perito(safe)

            indiv_ctx = {
                "kind": "perito",
                "perito": perito,
                "start": args.start,
                "end":   args.end,
                "add_comments": args.add_comments,
            }
            for script in SCRIPT_ORDER:
                script_file = script_path(script)
                planned_cmds.extend(build_commands_for_script(script_file, indiv_ctx))

            if args.r_appendix:
                planned_cmds.extend(build_r_commands_for_perito(perito, args.start, args.end, args.r_bin))

        # Coorte extra de %NC altíssima (opcional)
        extras_list = []
        if args.include_high_nc:
            df_high = pegar_peritos_nc_altissima(
                args.start, args.end,
                nc_threshold=args.high_nc_threshold,
                min_tasks=args.high_nc_min_tasks
            )
            if df_high.empty:
                print("Nenhum perito com %NC acima do limiar e mínimo de tarefas.")
            else:
                extras_list = [n for n in df_high['nomePerito'].tolist() if n not in set_top10]
                if extras_list:
                    print(f"Incluindo coorte extra (%NC ≥ {args.high_nc_threshold} e ≥ {args.high_nc_min_tasks} tarefas): {extras_list}")
                else:
                    print("Coorte extra presente, mas todos já estão no Top 10 — sem novos peritos.")
        else:
            print("Coorte extra de %NC alta desativada (--no-high-nc).")

        # Agenda scripts para a coorte extra
        for perito in extras_list:
            if not perito_tem_dados(perito, args.start, args.end):
                print(f"⚠️  Perito '{perito}' sem análises no período! Pulando.")
                continue

            safe = _safe(perito)
            _cleanup_exports_for_perito(safe)

            indiv_ctx = {
                "kind": "perito",
                "perito": perito,
                "start": args.start,
                "end":   args.end,
                "add_comments": args.add_comments,
            }
            for script in SCRIPT_ORDER:
                script_file = script_path(script)
                planned_cmds.extend(build_commands_for_script(script_file, indiv_ctx))

            if args.r_appendix:
                planned_cmds.extend(build_r_commands_for_perito(perito, args.start, args.end, args.r_bin))

        # Scripts globais (por período) — executar uma única vez ANTES
        try:
            for script in GLOBAL_SCRIPTS:
                script_file = script_path(script)
                planned_cmds[:0] = build_commands_for_global(script_file, args.start, args.end)
        except NameError:
            pass

    # =========================
    # PLANEJAMENTO — INDIVIDUAL
    # =========================
    else:
        perito = args.perito.strip()
        if not perito_tem_dados(perito, args.start, args.end):
            print(f"⚠️  Perito '{perito}' sem análises no período.")
            return

        safe = _safe(perito)
        _cleanup_exports_for_perito(safe)

        indiv_ctx = {
            "kind": "perito",
            "perito": perito,
            "start": args.start,
            "end":   args.end,
            "add_comments": args.add_comments,
        }
        for script in SCRIPT_ORDER:
            script_file = script_path(script)
            planned_cmds.extend(build_commands_for_script(script_file, indiv_ctx))

        if args.r_appendix:
            planned_cmds.extend(build_r_commands_for_perito(perito, args.start, args.end, args.r_bin))

        # Scripts globais (por período) — ainda executa para obter orgs-base
        try:
            for script in GLOBAL_SCRIPTS:
                script_file = script_path(script)
                planned_cmds[:0] = build_commands_for_global(script_file, args.start, args.end)
        except NameError:
            pass

    # Bootstrap de dependências R (entra antes de tudo)
    if args.r_appendix:
        cmd_boot = _r_deps_bootstrap_cmd(args.r_bin)
        if cmd_boot:
            planned_cmds.insert(0, cmd_boot)
        else:
            print("[AVISO] r_checks/_ensure_deps.R não encontrado; pulando bootstrap de pacotes.")

    # Apenas listar plano?
    if args.plan_only:
        print("\n===== PLANO DE EXECUÇÃO (dry-run) =====")
        for c in planned_cmds:
            print(" ".join(map(str, c)))
        print("=======================================\n")
        return

    # Execução dos comandos (Python e R) — sem wrapper
    for cmd in planned_cmds:
        try:
            if _is_r_cmd(cmd):
                print(f"[RUN] {' '.join(map(str, cmd))}")
                subprocess.run(cmd, check=False, cwd=RCHECK_DIR)
            else:
                print(f"[RUN] {' '.join(map(str, cmd))}")
                subprocess.run(cmd, check=False, env=_env_with_project_path(), cwd=SCRIPTS_DIR)
        except Exception as e:
            print(f"[ERRO] Falha executando: {' '.join(map(str, cmd))}\n  -> {e}")

    # Coleta saídas R (fallback) em EXPORT_DIR
    collect_r_outputs_to_export()

    # --------------------------------------------------------------------------
    # MONTAGEM DOS RELATÓRIOS E MOVIMENTAÇÃO DE ARTEFATOS
    # --------------------------------------------------------------------------
    org_paths = []
    extras_org_paths = []
    org_grupo_top10 = None
    org_to_export = None

    if args.top10:
        # Copia artefatos de grupo (inclui conversão de comments .md → .org)
        copiar_artefatos_top10(IMGS_DIR, COMMENTS_DIR, ORGS_DIR)
        # Copia artefatos do panorama W→WE (global)
        try:
            copiar_artefatos_weekday2weekend(IMGS_DIR, COMMENTS_DIR, ORGS_DIR)
        except NameError:
            pass

        # (Top10 grupo) Comentários GPT dos R checks (se habilitado)
        if args.r_appendix and args.add_comments:
            gerar_r_apendice_group_comments_if_possible(IMGS_DIR, COMMENTS_DIR, args.start, args.end)

        # Org do grupo
        org_grupo_top10 = gerar_org_top10_grupo(args.start, args.end, RELATORIO_DIR, IMGS_DIR, COMMENTS_DIR)

        # Peritos do Top10
        peritos_df = pegar_10_piores_peritos(args.start, args.end, min_analises=args.min_analises)
        lista_top10 = peritos_df['nomePerito'].tolist()
        for perito in lista_top10:
            if not perito_tem_dados(perito, args.start, args.end):
                continue
            copiar_artefatos_perito(perito, IMGS_DIR, COMMENTS_DIR, ORGS_DIR)
            if args.r_appendix and args.add_comments:
                gerar_r_apendice_comments_if_possible(perito, IMGS_DIR, COMMENTS_DIR, args.start, args.end)
            org_path = gerar_org_perito(perito, args.start, args.end, args.add_comments, IMGS_DIR, COMMENTS_DIR, RELATORIO_DIR)
            org_paths.append(org_path)

        # Coorte extra
        if args.include_high_nc:
            df_high = pegar_peritos_nc_altissima(args.start, args.end,
                                                 nc_threshold=args.high_nc_threshold,
                                                 min_tasks=args.high_nc_min_tasks)
            set_top10 = set(lista_top10)
            extras_list = [n for n in df_high['nomePerito'].tolist() if n not in set_top10]
            for perito in extras_list:
                if not perito_tem_dados(perito, args.start, args.end):
                    continue
                copiar_artefatos_perito(perito, IMGS_DIR, COMMENTS_DIR, ORGS_DIR)
                if args.r_appendix and args.add_comments:
                    gerar_r_apendice_comments_if_possible(perito, IMGS_DIR, COMMENTS_DIR, args.start, args.end)
                org_path = gerar_org_perito(perito, args.start, args.end, args.add_comments, IMGS_DIR, COMMENTS_DIR, RELATORIO_DIR)
                extras_org_paths.append(org_path)

        # ← NOVO: mover quaisquer .md remanescentes para 'markdown/'
        _mover_markdowns_de_exports(MARKDOWN_DIR)

        # Org consolidado do Top10 (com panorama global ao final + reincidentes)
        if (args.export_org or args.export_pdf) and (org_paths or org_grupo_top10 or extras_org_paths):
            org_final = os.path.join(RELATORIO_DIR, f"relatorio_dez_piores_{args.start}_a_{args.end}.org")
            lines = [f"* Relatório dos 10 piores peritos ({args.start} a {args.end})", ""]

            if org_grupo_top10 and os.path.exists(org_grupo_top10):
                with open(org_grupo_top10, encoding="utf-8") as f:
                    lines.append(f.read().strip())
                    lines.append("#+LATEX: \\newpage\n")

            for org_path in org_paths:
                with open(org_path, encoding="utf-8") as f:
                    content = f.read().strip()
                    if content:
                        lines.append(content)
                        lines.append("#+LATEX: \\newpage\n")

            if extras_org_paths:
                lines.append(f"** Peritos com %NC ≥ {args.high_nc_threshold:.0f}% e ≥ {args.high_nc_min_tasks} tarefas\n")
                for org_path in extras_org_paths:
                    with open(org_path, encoding="utf-8") as f:
                        content = f.read().strip()
                        if content:
                            content = shift_org_headings(content, delta=1)
                            lines.append(content)
                            lines.append("#+LATEX: \\newpage\n")

            # Panorama global (W→WE) AO FINAL do Top10
            _append_weekday2weekend_panorama_block(
                lines, IMGS_DIR, COMMENTS_DIR,
                start=args.start, end=args.end,
                heading_level="**"
            )

            with open(org_final, "w", encoding="utf-8") as f:
                f.write("\n".join(lines).strip() + "\n")
            print(f"✅ Org consolidado salvo em: {org_final}")

            # Reincidentes (≥2 meses no Top 10) — agora via DB Jan→end e com min_analises do CLI
            reinc_csv = os.path.join(ORGS_DIR, "top10_reincidentes.csv")
            _run_reincidentes_scan(
                reinc_csv,
                min_months=2,
                root=OUTPUTS_DIR,
                start=args.start,
                end=args.end,
                min_analises=args.min_analises
            )
            _append_reincidentes_to_org(org_final, reinc_csv, min_months=2, heading_level="**")

            org_to_export = org_final
        else:
            org_to_export = org_paths[0] if org_paths else None

    # --------------------------------------------------------------------------
    # INDIVIDUAL — consolidado + bloco W→WE condicional por perito
    # --------------------------------------------------------------------------
    else:
        perito = args.perito.strip()

        imgs_dir_i     = os.path.join(RELATORIO_DIR, "imgs")
        comments_dir_i = os.path.join(RELATORIO_DIR, "comments")
        orgs_dir_i     = os.path.join(RELATORIO_DIR, "orgs")

        copiar_artefatos_perito(perito, imgs_dir_i, comments_dir_i, orgs_dir_i)

        # Copia artefatos globais pois o bloco W→WE do perito será extraído deles
        try:
            copiar_artefatos_weekday2weekend(imgs_dir_i, comments_dir_i, orgs_dir_i)
        except NameError:
            pass

        # ← NOVO: mover quaisquer .md remanescentes para 'markdown/'
        _mover_markdowns_de_exports(MARKDOWN_DIR)

        # Monta org individual base (sem panorama no topo)
        perito_org_path = gerar_org_perito(
            perito, args.start, args.end, args.add_comments,
            imgs_dir_i, comments_dir_i, RELATORIO_DIR
        )

        # Consolida: relatório do perito + (se houver) bloco W→WE deste perito no FINAL
        org_final = os.path.join(
            RELATORIO_DIR, f"relatorio_{_safe(perito)}_{args.start}_a_{args.end}.org"
        )
        lines = [f"* Relatório individual — {perito} ({args.start} a {args.end})", ""]

        with open(perito_org_path, encoding="utf-8") as f:
            content = f.read().strip()
            if content:
                lines.append(content)
                lines.append("#+LATEX: \\newpage\n")

        # Acrescenta bloco W→WE apenas se o perito tiver casos
        _append_weekday2weekend_perito_block_if_any(
            lines, perito, imgs_dir_i, comments_dir_i,
            start=args.start, end=args.end,
            heading_level="**"
        )

        with open(org_final, "w", encoding="utf-8") as f:
            f.write("\n".join(lines).strip() + "\n")

        org_to_export = org_final

    # Exportação para PDF (opcional) + capa
    if args.export_pdf and org_to_export:
        pdf_path = exportar_org_para_pdf(org_to_export, font="DejaVu Sans")
        if pdf_path and os.path.exists(pdf_path):
            adicionar_capa_pdf(pdf_path)

    # Tempo total
    dt = time.time() - t0
    mm, ss = divmod(int(dt + 0.5), 60)
    hh, mm = divmod(mm, 60)
    print(f"⏱️ Tempo total: {hh:02d}:{mm:02d}:{ss:02d}")


if __name__ == '__main__':
    main()
